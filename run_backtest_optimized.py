#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä¼˜åŒ–å›æµ‹è„šæœ¬ - æ”¯æŒå‘½ä»¤è¡Œå‚æ•°ï¼ˆGitHub Actions å…¼å®¹ç‰ˆï¼‰
ç”¨æ³•:
  å•æ—¥å›æµ‹: python run_backtest_optimized.py --mode single --date 2025-09-25 --hold 1
  å¤šæ—¥å›æµ‹: python run_backtest_optimized.py --mode multi --start 2025-09-23 --end 2025-09-27 --hold 1
"""

import sys
import os
import argparse

# è®¾ç½®æ§åˆ¶å°ç¼–ç ä¸ºUTF-8
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
import akshare as ak
import pandas as pd
from datetime import datetime, timedelta
import logging
import json
import time
import pickle

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.data.data_fetcher import StockDataFetcher
from src.analysis.stock_filter import StockFilter
from config.backtest_config import BACKTEST_FILTER_CONFIG, BACKTEST_SAMPLE_CONFIG

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class OptimizedBacktest:
    """ä¼˜åŒ–å›æµ‹ç³»ç»Ÿ - ä½¿ç”¨å®½æ¾é…ç½®"""

    def __init__(self):
        self.data_fetcher = StockDataFetcher()
        self.stock_filter = StockFilter(config=BACKTEST_FILTER_CONFIG)
        self.cache_dir = './cache'
        os.makedirs(self.cache_dir, exist_ok=True)
        self.stock_name_cache = {}

    def get_cache_file(self, cache_key):
        return os.path.join(self.cache_dir, f"{cache_key}.pkl")

    def load_from_cache(self, cache_key):
        cache_file = self.get_cache_file(cache_key)
        if os.path.exists(cache_file):
            try:
                expire_days = BACKTEST_SAMPLE_CONFIG.get('cache_expire_days', 7)
                if time.time() - os.path.getmtime(cache_file) < (expire_days * 86400):
                    with open(cache_file, 'rb') as f:
                        return pickle.load(f)
            except:
                pass
        return None

    def save_to_cache(self, cache_key, data):
        cache_file = self.get_cache_file(cache_key)
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(data, f)
        except Exception as e:
            logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

    def get_stock_name(self, stock_code: str):
        if stock_code in self.stock_name_cache:
            return self.stock_name_cache[stock_code]
        try:
            info = ak.stock_individual_info_em(symbol=stock_code)
            if not info.empty:
                name = info[info['item'] == 'è‚¡ç¥¨ç®€ç§°']['value'].values
                if len(name) > 0:
                    stock_name = str(name[0])
                    self.stock_name_cache[stock_code] = stock_name
                    return stock_name
        except:
            pass
        return f'è‚¡ç¥¨{stock_code}'

    def get_csi300_stocks(self):
        cache_key = "csi300_stocks_with_names"
        cached_data = self.load_from_cache(cache_key)
        if cached_data:
            logger.info(f"ä»ç¼“å­˜åŠ è½½æ²ªæ·±300æˆåˆ†è‚¡: {len(cached_data.get('stocks', []))} åª")
            if isinstance(cached_data, dict):
                self.stock_name_cache = cached_data.get('name_cache', {})
                return cached_data.get('stocks', [])
            return cached_data

        stocks = []
        try:
            logger.info("æ­£åœ¨è·å–æ²ªæ·±300æˆåˆ†è‚¡åˆ—è¡¨ï¼ˆæ–¹æ³•1ï¼šakshareï¼‰...")
            csi300 = ak.index_stock_cons(symbol="000300")
            stocks = csi300['å“ç§ä»£ç '].tolist()
            for i, code in enumerate(stocks):
                name = csi300[csi300['å“ç§ä»£ç '] == code]['å“ç§åç§°'].values
                if len(name) > 0:
                    self.stock_name_cache[code] = str(name[0])
            logger.info(f"âœ… æˆåŠŸè·å– {len(stocks)} åªæ²ªæ·±300æˆåˆ†è‚¡")
        except Exception as e:
            logger.warning(f"âš ï¸ æ–¹æ³•1å¤±è´¥: {e}")
            try:
                csi300 = ak.index_stock_cons_csindex(symbol="000300")
                stocks = csi300['æˆåˆ†åˆ¸ä»£ç '].tolist()
                logger.info(f"âœ… æ–¹æ³•2æˆåŠŸè·å– {len(stocks)} åªè‚¡ç¥¨")
            except Exception as e2:
                logger.error(f"âŒ æ‰€æœ‰æ–¹æ³•å‡å¤±è´¥: {e2}")

        if not stocks:
            return []

        cache_data = {'stocks': stocks, 'name_cache': self.stock_name_cache}
        self.save_to_cache(cache_key, cache_data)
        return stocks

    def get_stock_data_for_date(self, stock_code: str, date: str, pe_ratio: float = None, purpose: str = "buy"):
        cache_key = f"stock_{stock_code}_{date}_{purpose}"
        cached_data = self.load_from_cache(cache_key)
        if cached_data:
            if pe_ratio is not None and pe_ratio > 0:
                cached_data['pe_ratio'] = pe_ratio
            return cached_data

        try:
            end_date = (datetime.strptime(date, '%Y-%m-%d') + timedelta(days=5)).strftime('%Y%m%d')
            start_date = (datetime.strptime(date, '%Y-%m-%d') - timedelta(days=35)).strftime('%Y%m%d')

            df = ak.stock_zh_a_hist(symbol=stock_code, period="daily",
                                   start_date=start_date, end_date=end_date, adjust="qfq")
            if df.empty:
                return None

            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
            target_date = pd.to_datetime(date)
            df_on_date = df[df['æ—¥æœŸ'] <= target_date]
            if df_on_date.empty:
                return None

            row = df_on_date.iloc[-1]
            stock_name = self.get_stock_name(stock_code)

            final_pe_ratio = 20.0
            if pe_ratio is not None and pe_ratio > 0:
                final_pe_ratio = pe_ratio
            elif 'å¸‚ç›ˆç‡' in row:
                try:
                    pe_value = row['å¸‚ç›ˆç‡']
                    if pe_value and pe_value > 0:
                        final_pe_ratio = float(pe_value)
                except:
                    pass

            momentum_20d = 0
            if len(df_on_date) >= 20:
                momentum_20d = (row['æ”¶ç›˜'] / df_on_date.iloc[-20]['æ”¶ç›˜'] - 1) * 100
            elif len(df_on_date) >= 2:
                momentum_20d = (row['æ”¶ç›˜'] / df_on_date.iloc[0]['æ”¶ç›˜'] - 1) * 100

            data = {
                'code': stock_code,
                'name': stock_name,
                'price': float(row['æ”¶ç›˜']),
                'change_pct': float(row['æ¶¨è·Œå¹…']),
                'volume': int(row['æˆäº¤é‡']),
                'turnover': float(row['æˆäº¤é¢']),
                'pe_ratio': final_pe_ratio,
                'momentum_20d': momentum_20d,
                'strength_score': 0
            }
            self.save_to_cache(cache_key, data)
            return data

        except Exception as e:
            logger.debug(f"è·å– {stock_code} æ•°æ®å¤±è´¥: {e}")
            return None

    def get_next_trading_day(self, date: str, days: int = 1):
        try:
            current = datetime.strptime(date, '%Y-%m-%d')
            for i in range(1, 15):
                next_date = current + timedelta(days=i*days)
                if next_date.weekday() < 5:
                    return next_date.strftime('%Y-%m-%d')
            return (current + timedelta(days=days)).strftime('%Y-%m-%d')
        except:
            return date

    def fetch_pe_ratios_batch(self, stock_codes: list) -> dict:
        pe_dict = {}
        logger.info("ğŸ“Š æ­£åœ¨è·å–PEæ•°æ®...")
        success_count = 0
        for i, code in enumerate(stock_codes):
            try:
                stock_data = self.data_fetcher.get_stock_realtime_data(code)
                if stock_data and stock_data.get('pe_ratio'):
                    pe_dict[code] = stock_data['pe_ratio']
                    success_count += 1
                if (i + 1) % 50 == 0:
                    logger.info(f"   è¿›åº¦: {i+1}/{len(stock_codes)}")
            except:
                pass
        logger.info(f"âœ… æˆåŠŸè·å– {success_count}/{len(stock_codes)} åªè‚¡ç¥¨çš„PEæ•°æ®")
        return pe_dict

    def backtest_single_day(self, analysis_date: str, hold_days: int = 1):
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ“… å›æµ‹æ—¥æœŸ: {analysis_date} | æŒæœ‰{hold_days}å¤©")
        logger.info(f"{'='*70}")

        stock_list = self.get_csi300_stocks()
        if not stock_list:
            logger.error("æ— æ³•è·å–æ²ªæ·±300æˆåˆ†è‚¡åˆ—è¡¨")
            return None

        sample_size = BACKTEST_SAMPLE_CONFIG['sample_size']
        if sample_size >= len(stock_list):
            sampled_stocks = stock_list
        else:
            import random
            random.seed(BACKTEST_SAMPLE_CONFIG['random_seed'])
            sampled_stocks = random.sample(stock_list, min(sample_size, len(stock_list)))

        logger.info(f"ğŸ“Š åˆ†æè‚¡ç¥¨æ•°: {len(sampled_stocks)} åª")
        pe_ratios = self.fetch_pe_ratios_batch(sampled_stocks)

        stock_data = []
        for i, code in enumerate(sampled_stocks):
            if i % 20 == 0:
                logger.info(f"â³ æ•°æ®è·å–è¿›åº¦: {i+1}/{len(sampled_stocks)}")
            data = self.get_stock_data_for_date(code, analysis_date, pe_ratios.get(code), "buy")
            if data:
                stock_data.append(data)
            if i % 20 == 19:
                time.sleep(0.3)

        logger.info(f"âœ… æˆåŠŸè·å– {len(stock_data)} åªè‚¡ç¥¨æ•°æ®")

        if len(stock_data) < 10:
            logger.error("âŒ è·å–çš„è‚¡ç¥¨æ•°æ®å¤ªå°‘")
            return None

        selected_stocks = self.stock_filter.select_top_stocks(stock_data)

        if not selected_stocks:
            logger.warning("âš ï¸  æœªç­›é€‰å‡ºä»»ä½•è‚¡ç¥¨")
            return {'analysis_date': analysis_date, 'selected_count': 0, 'performance': []}

        # å»é‡
        unique_stocks, seen_codes = [], set()
        for stock in selected_stocks:
            if stock['code'] not in seen_codes:
                unique_stocks.append(stock)
                seen_codes.add(stock['code'])
        selected_stocks = unique_stocks

        logger.info(f"\nğŸ† ç­›é€‰ç»“æœ ({len(selected_stocks)}åª):")
        for stock in selected_stocks:
            logger.info(f"   #{stock['rank']} {stock['name']} ({stock['code']}): "
                       f"Â¥{stock['price']:.2f}, PE={stock.get('pe_ratio', 0):.1f}, "
                       f"åˆ†æ•°={stock.get('strength_score', 0):.0f}")

        sell_date = self.get_next_trading_day(analysis_date, hold_days)
        logger.info(f"\nğŸ’° æ”¶ç›Šè®¡ç®— (å–å‡ºæ—¥æœŸ: {sell_date}):")

        performance = []
        avg_return = max_return = min_return = win_rate = win_count = 0

        for stock in selected_stocks:
            sell_data = self.get_stock_data_for_date(stock['code'], sell_date, purpose="sell")
            if sell_data:
                buy_price = stock['price']
                sell_price = sell_data['price']
                return_pct = (sell_price / buy_price - 1) * 100
                performance.append({
                    'code': stock['code'],
                    'name': stock['name'],
                    'buy_price': buy_price,
                    'sell_price': sell_price,
                    'return_pct': return_pct,
                    'pe_ratio': stock.get('pe_ratio', 0),
                    'strength_score': stock.get('strength_score', 0)
                })
                emoji = "ğŸ“ˆ" if return_pct > 0 else "ğŸ“‰" if return_pct < 0 else "â–"
                logger.info(f"   {emoji} {stock['name']}: Â¥{buy_price:.2f} â†’ Â¥{sell_price:.2f} ({return_pct:+.2f}%)")

        if performance:
            returns = [p['return_pct'] for p in performance]
            avg_return = sum(returns) / len(returns)
            max_return = max(returns)
            min_return = min(returns)
            win_count = len([r for r in returns if r > 0])
            win_rate = win_count / len(returns) * 100
            logger.info(f"\nğŸ“Š ç­–ç•¥è¡¨ç°:")
            logger.info(f"   â€¢ å¹³å‡æ”¶ç›Š: {avg_return:+.2f}%")
            logger.info(f"   â€¢ æ”¶ç›ŠåŒºé—´: {min_return:+.2f}% ~ {max_return:+.2f}%")
            logger.info(f"   â€¢ èƒœç‡: {win_rate:.1f}% ({win_count}/{len(returns)})")

        return {
            'analysis_date': analysis_date,
            'sell_date': sell_date,
            'hold_days': hold_days,
            'selected_count': len(selected_stocks),
            'performance': performance,
            'summary': {
                'avg_return': avg_return,
                'max_return': max_return,
                'min_return': min_return,
                'win_rate': win_rate,
                'win_count': win_count,
                'total_count': len(performance)
            }
        }

    def backtest_multi_days(self, start_date: str, end_date: str, hold_days: int = 1):
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ“… å¤šæ—¥å›æµ‹: {start_date} ~ {end_date}")
        logger.info(f"{'='*70}")

        trading_days = []
        current = datetime.strptime(start_date, '%Y-%m-%d')
        end = datetime.strptime(end_date, '%Y-%m-%d')
        while current <= end:
            if current.weekday() < 5:
                trading_days.append(current.strftime('%Y-%m-%d'))
            current += timedelta(days=1)

        logger.info(f"å…± {len(trading_days)} ä¸ªäº¤æ˜“æ—¥")

        all_results = []
        for i, date in enumerate(trading_days):
            logger.info(f"\nè¿›åº¦: {i+1}/{len(trading_days)}")
            result = self.backtest_single_day(date, hold_days)
            if result and result['selected_count'] > 0:
                all_results.append(result)
            time.sleep(1)

        if all_results:
            all_returns = []
            for r in all_results:
                all_returns.extend([p['return_pct'] for p in r['performance']])
            logger.info(f"\n{'='*70}")
            logger.info(f"ğŸ“Š å›æµ‹æ€»ç»“:")
            logger.info(f"   â€¢ æœ‰æ•ˆäº¤æ˜“æ—¥: {len(all_results)}")
            logger.info(f"   â€¢ æ€»äº¤æ˜“æ¬¡æ•°: {len(all_returns)}")
            if all_returns:
                win_count = len([r for r in all_returns if r > 0])
                logger.info(f"   â€¢ æ•´ä½“å¹³å‡æ”¶ç›Š: {sum(all_returns)/len(all_returns):+.2f}%")
                logger.info(f"   â€¢ æœ€ä½³å•ç¬”: {max(all_returns):+.2f}%")
                logger.info(f"   â€¢ æœ€å·®å•ç¬”: {min(all_returns):+.2f}%")
                logger.info(f"   â€¢ æ•´ä½“èƒœç‡: {win_count/len(all_returns)*100:.1f}%")

        return all_results


def main():
    # â”€â”€ å‘½ä»¤è¡Œå‚æ•°è§£æï¼ˆGitHub Actions å…¼å®¹ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parser = argparse.ArgumentParser(description='æ²ªæ·±300ç­–ç•¥å›æµ‹ç³»ç»Ÿ')
    parser.add_argument('--mode', choices=['single', 'multi'], default='single',
                        help='å›æµ‹æ¨¡å¼: single=å•æ—¥, multi=å¤šæ—¥')
    parser.add_argument('--date', type=str, default=None,
                        help='å•æ—¥å›æµ‹æ—¥æœŸï¼Œæ ¼å¼: 2025-09-25')
    parser.add_argument('--start', type=str, default=None,
                        help='å¤šæ—¥å›æµ‹å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼: 2025-09-23')
    parser.add_argument('--end', type=str, default=None,
                        help='å¤šæ—¥å›æµ‹ç»“æŸæ—¥æœŸï¼Œæ ¼å¼: 2025-09-27')
    parser.add_argument('--hold', type=int, default=1,
                        help='æŒæœ‰å¤©æ•°ï¼Œé»˜è®¤1å¤©')
    args = parser.parse_args()

    print("="*70)
    print("ğŸ“Š æ²ªæ·±300ç­–ç•¥å›æµ‹ç³»ç»Ÿ")
    print("="*70)

    backtest = OptimizedBacktest()
    os.makedirs('./logs/backtest', exist_ok=True)

    if args.mode == 'single':
        # æœªä¼ æ—¥æœŸæ—¶é»˜è®¤ç”¨ä¸Šä¸€ä¸ªå·¥ä½œæ—¥
        if args.date is None:
            today = datetime.today()
            offset = max(1, (today.weekday() + 6) % 7 - 3)  # å‘¨ä¸€å–ä¸Šå‘¨äº”
            last_workday = today - timedelta(days=offset if today.weekday() == 0 else 1)
            args.date = last_workday.strftime('%Y-%m-%d')
            print(f"æœªæŒ‡å®šæ—¥æœŸï¼Œè‡ªåŠ¨ä½¿ç”¨ä¸Šä¸€å·¥ä½œæ—¥: {args.date}")

        result = backtest.backtest_single_day(args.date, args.hold)
        if result:
            filename = f"./logs/backtest/backtest_{args.date}_{args.hold}days.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            logger.info(f"\nâœ… å›æµ‹ç»“æœå·²ä¿å­˜: {filename}")

    elif args.mode == 'multi':
        if not args.start or not args.end:
            print("âŒ å¤šæ—¥å›æµ‹éœ€è¦æŒ‡å®š --start å’Œ --end å‚æ•°")
            sys.exit(1)

        results = backtest.backtest_multi_days(args.start, args.end, args.hold)
        if results:
            filename = f"./logs/backtest/backtest_{args.start}_to_{args.end}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"\nâœ… å›æµ‹ç»“æœå·²ä¿å­˜: {filename}")


if __name__ == "__main__":
    main()
